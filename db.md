# üóÑÔ∏è ROADMAP WIZARD BANCO DE DADOS - DOM√çNIO ABSOLUTO
*Do Zero ao Database Architect/Research-Level em Todas as √Åreas*

---

## **FASE -1: FUNDAMENTOS COMPUTACIONAIS (1-2 meses)**
*Para quem quer come√ßar do ZERO TOTAL*

### **-1.1 Sistemas Operacionais B√°sicos**
- **Linux fundamentals**: Navega√ß√£o terminal, permiss√µes, processos
- **File systems**: Como dados s√£o armazenados em disco (ext4, NTFS, APFS)
- **Mem√≥ria e processos**: RAM, swap, gerenciamento de recursos
- **Networking b√°sico**: IP, portas, TCP/UDP, DNS

### **-1.2 Linguagens de Programa√ß√£o Base**
- **SQL b√°sico**: SELECT, INSERT, UPDATE, DELETE - flu√™ncia total
- **Python/JavaScript**: Para scripts, automa√ß√£o e testes
- **Estruturas de dados**: Arrays, listas, hashmaps, √°rvores
- **Algoritmos b√°sicos**: Busca, ordena√ß√£o, complexidade O(n)

### **-1.3 Modelagem de Dados Conceitual**
- **Entidades e relacionamentos**: O que √© um "dado"?
- **Chaves**: Prim√°rias, estrangeiras, √∫nicas
- **Normaliza√ß√£o b√°sica**: 1NF, 2NF, 3NF intuitivamente
- **Diagramas ER**: Leitura e cria√ß√£o b√°sica

### **üìö Recursos Fase -1:**
1. **"Learning SQL" - Alan Beaulieu** (SQL do zero)
2. **"SQL Queries for Mere Mortals" - John Viescas**
3. **Linux Command Line Basics** (tutoriais online)
4. **"Grokking Algorithms" - Aditya Bhargava** (funda√ß√£o CS)

### **‚ö° Teste de Prontid√£o para Fase 0:**
```bash
‚úÖ Escreve queries SELECT com JOINs sem pensar
‚úÖ Cria scripts b√°sicos em Python/JS
‚úÖ Entende como arquivos s√£o armazenados em disco
‚úÖ Modela entidades simples (users, posts, comments)
‚úÖ Explica diferen√ßa entre PRIMARY KEY e FOREIGN KEY
```

---

## **FASE 0: SQL E MODELAGEM RELACIONAL PROFUNDA (2-3 meses)**
*A funda√ß√£o que 90% pula e sofre depois*

### **0.1 SQL Avan√ßado - Dom√≠nio Total**
- **Queries complexas**: CTEs recursivos, window functions, pivots
- **Subqueries**: Correlacionadas, EXISTS, IN vs JOIN performance
- **Set operations**: UNION, INTERSECT, EXCEPT
- **Transa√ß√µes**: ACID, isolation levels, deadlocks
- **Stored procedures e triggers**: Quando usar, quando evitar

### **0.2 Modelagem Relacional Rigorosa**
- **Normaliza√ß√£o profunda**: BCNF, 4NF, 5NF - quando e por qu√™
- **Desnormaliza√ß√£o estrat√©gica**: Trade-offs, quando quebrar regras
- **Integridade referencial**: Cascatas, constraints complexos
- **Design patterns**: Temporal data, hierarchical data, EAV (e por que evitar)
- **Modelagem dimensional**: Star schema, snowflake schema

### **0.3 Teoria Relacional Formal**
- **√Ålgebra relacional**: Opera√ß√µes fundamentais, equival√™ncias
- **C√°lculo relacional**: Tuplas, dom√≠nios
- **Depend√™ncias funcionais**: Armstrong's axioms, closure
- **Teoria da normaliza√ß√£o**: Decomposi√ß√£o sem perdas

### **üìö Livros Fase 0:**
1. **"SQL Performance Explained" - Markus Winand** (ESSENCIAL)
2. **"Database Design for Mere Mortals" - Michael Hernandez**
3. **"The Art of SQL" - St√©phane Faroult**
4. **"Database System Concepts" - Silberschatz** (cap√≠tulos 1-8)

---

## **FASE 1: ARQUITETURA DE SGBD (4-6 meses)**
*Como bancos de dados realmente funcionam por dentro*

### **1.1 Storage Engine Internals**
- **File organization**: Heap files, sorted files, hashed files
- **Page structure**: Slotted pages, record formats
- **Buffer pool**: Gerenciamento de mem√≥ria, page replacement (LRU, Clock)
- **Disk I/O**: Sequencial vs rand√¥mico, SSD vs HDD, lat√™ncias

### **1.2 Indexa√ß√£o Profunda**
- **B-Trees e B+Trees**: Implementa√ß√£o completa, inser√ß√£o, dele√ß√£o, splits
- **Hash indexes**: Extendible hashing, linear hashing
- **Bitmap indexes**: Quando usar, compress√£o
- **Full-text indexes**: Inverted indexes, tokeniza√ß√£o
- **Spatial indexes**: R-trees, quadtrees, geohashing

### **1.3 Query Processing**
- **Parser e optimizer**: Como SQL vira execution plan
- **Algoritmos de JOIN**: Nested loop, hash join, sort-merge join
- **Cost-based optimization**: Statistics, cardinality estimation
- **Execution models**: Iterator model, vectorization, compilation

### **1.4 Transaction Management**
- **Concurrency control**: 2PL, MVCC, OCC
- **Isolation levels**: Read uncommitted ‚Üí Serializable (DEEP understanding)
- **Deadlock detection**: Wait-for graph, timeouts
- **Recovery**: WAL (Write-Ahead Logging), ARIES algorithm
- **Checkpointing**: Estrat√©gias, trade-offs

### **üìö Livros Fase 1:**
1. **"Database Internals" - Alex Petrov** (OBRA-PRIMA moderna)
2. **"Transaction Processing" - Jim Gray & Andreas Reuter** (cl√°ssico)
3. **"Database System Implementation" - Garcia-Molina**
4. **"Readings in Database Systems" (Red Book)** - Hellerstein & Stonebraker

### **üõ†Ô∏è Projetos Pr√°ticos Fase 1:**
```
- Implementar B+Tree do zero em C/Rust
- Criar buffer pool manager simples
- Implementar 2PL com deadlock detection
- Analisar query plans de DBs reais (PostgreSQL EXPLAIN ANALYZE)
```

---

## **FASE 2: SISTEMAS DISTRIBU√çDOS E ESCALA (6-8 meses)**
*Como bancos funcionam em produ√ß√£o real*

### **2.1 Teorema CAP e Trade-offs**
- **CAP theorem**: Demonstra√ß√£o, implica√ß√µes pr√°ticas
- **PACELC**: Extens√£o do CAP, lat√™ncia vs consist√™ncia
- **Consistency models**: Linearizability, sequential, eventual, causal
- **Replica√ß√£o**: Master-slave, master-master, quorum-based

### **2.2 Sharding e Particionamento**
- **Estrat√©gias**: Hash-based, range-based, directory-based
- **Rebalancing**: Consistent hashing, virtual nodes
- **Cross-shard queries**: Scatter-gather, 2PC
- **Hotspots**: Detec√ß√£o e mitiga√ß√£o

### **2.3 Consenso Distribu√≠do**
- **Paxos**: Algoritmo completo, implementa√ß√£o
- **Raft**: Mais simples que Paxos, garantias
- **ZooKeeper/etcd**: Uso pr√°tico de consenso
- **Byzantine Fault Tolerance**: PBFT, blockchain foundations

### **2.4 Distributed Transactions**
- **2PC (Two-Phase Commit)**: Problemas, coordinator failure
- **3PC**: Melhorias sobre 2PC
- **Sagas**: Compensating transactions, orquestra√ß√£o vs coreografia
- **TCC (Try-Confirm-Cancel)**: Implementa√ß√£o pr√°tica

### **üìö Livros Fase 2:**
1. **"Designing Data-Intensive Applications" - Martin Kleppmann** (B√çBLIA)
2. **"Database Reliability Engineering" - Laine Campbell**
3. **"Distributed Systems" - Maarten van Steen & Andrew Tanenbaum**
4. **Papers**: Google Spanner, Amazon DynamoDB, Facebook TAO

---

## **FASE 3: NoSQL E MODELOS ALTERNATIVOS (5-6 meses)**
*Al√©m do modelo relacional*

### **3.1 Key-Value Stores**
- **Arquitetura**: LSM-trees, SSTables, compaction
- **Redis profundo**: Data structures, persistence (RDB, AOF), cluster
- **Memcached**: Consistente hashing, eviction policies
- **RocksDB**: Internals, tuning, use cases

### **3.2 Document Databases**
- **MongoDB**: WiredTiger, sharding, replica sets, aggregation pipeline
- **CouchDB**: MVCC, map-reduce views, eventual consistency
- **Schema design**: Embedded vs referenced, denormalization patterns
- **Indexa√ß√£o**: Single field, compound, multikey, text, geospatial

### **3.3 Column-Family Stores**
- **Cassandra**: Ring architecture, gossip protocol, tunable consistency
- **HBase**: Architecture sobre HDFS, regions, compactions
- **BigTable model**: Wide-column, row keys, column families
- **Time-series optimization**: Schema design para s√©ries temporais

### **3.4 Graph Databases**
- **Neo4j**: Cypher, native graph storage, index-free adjacency
- **Property graph model**: Nodes, edges, properties
- **RDF/Triple stores**: SPARQL, semantic web
- **Graph algorithms**: Shortest path, PageRank, community detection

### **üìö Livros Fase 3:**
1. **"NoSQL Distilled" - Pramod Sadalage & Martin Fowler**
2. **"Seven Databases in Seven Weeks" - Luc Perkins**
3. **"The Practitioner's Guide to Graph Data" - Denise Gosnell**
4. **Papers**: Cassandra (Facebook), Dynamo (Amazon), BigTable (Google)

### **üõ†Ô∏è Projetos Pr√°ticos Fase 3:**
```
- Implementar LSM-tree simplificado
- Criar sistema de recomenda√ß√£o com Neo4j
- Modelar dados de gaming (invent√°rio, players) em MongoDB
- Implementar time-series database toy
```

---

## **FASE 4: PERFORMANCE E OTIMIZA√á√ÉO EXTREMA (6-8 meses)**
*Como fazer bancos voarem*

### **4.1 Query Optimization Avan√ßada**
- **Estat√≠sticas**: Histogramas, sketches (HyperLogLog, Count-Min)
- **Join ordering**: Dynamic programming, greedy algorithms
- **Rewrite rules**: Predicate pushdown, projection pruning
- **Adaptive query processing**: Re-optimization em runtime
- **Hints e force indexes**: Quando substituir o optimizer

### **4.2 Indexa√ß√£o Avan√ßada**
- **Covering indexes**: Minimizando table lookups
- **Partial indexes**: Filtrando √≠ndices, economia de espa√ßo
- **Expression indexes**: Fun√ß√µes, JSON fields
- **Index maintenance**: Fill factor, fragmentation
- **Bloom filters**: Evitando disk seeks desnecess√°rios

### **4.3 Caching Strategies**
- **Query result cache**: Invalida√ß√£o, TTL
- **Application-level cache**: Redis patterns, cache-aside, write-through
- **Materialized views**: Refresh strategies, incremental updates
- **CDN para dados**: Edge caching, geolocaliza√ß√£o

### **4.4 Hardware e Tuning**
- **CPU**: SIMD, vectoriza√ß√£o de queries
- **Mem√≥ria**: Page cache, huge pages, NUMA awareness
- **Disk**: I/O scheduler, RAID levels, NVMe
- **Network**: Lat√™ncia inter-datacenter, compression

### **4.5 Profiling e Monitoring**
- **Query analysis**: EXPLAIN, execution time breakdown
- **Slow query logs**: Identifica√ß√£o de problemas
- **Metrics**: QPS, lat√™ncia p50/p95/p99, connection pool
- **Tools**: pgBadger, pt-query-digest, Prometheus+Grafana

### **üìö Livros Fase 4:**
1. **"High Performance MySQL" - Baron Schwartz** (tamb√©m vale pra outros DBs)
2. **"PostgreSQL Query Optimization" - Henrietta Dombrovskaya**
3. **"Systems Performance" - Brendan Gregg**
4. **"Database Performance at Scale" - Felipe Cardeneti Mendes**

---

## **FASE 5: DATA WAREHOUSING E ANALYTICS (5-6 meses)**
*Big data e analytics em escala*

### **5.1 OLAP vs OLTP**
- **Diferen√ßas arquiteturais**: Row-store vs column-store
- **ETL/ELT**: Extract, Transform, Load - pipelines
- **Data lakes**: Parquet, ORC, Avro formats
- **Lambda architecture**: Batch + stream processing

### **5.2 Column-Oriented Databases**
- **ClickHouse**: Merge tree engine, distributed queries, materializa√ß√£o
- **Apache Druid**: Real-time analytics, ingestion
- **Vertica**: Projections, compression (RLE, dictionary)
- **Snowflake**: Architecture (storage + compute separation), time travel

### **5.3 MPP (Massively Parallel Processing)**
- **Redshift**: Dist keys, sort keys, vacuum, analyze
- **BigQuery**: Capacitor storage, Dremel execution
- **Presto/Trino**: Query federation, connectors
- **Spark SQL**: Catalyst optimizer, Tungsten execution

### **5.4 Data Modeling para Analytics**
- **Star schema avan√ßado**: Fact tables, dimension tables, slowly changing dimensions
- **Aggregation tables**: Pre-computed summaries
- **Cube design**: OLAP cubes, roll-up, drill-down
- **Data vault**: Para data warehouses de longo prazo

### **üìö Livros Fase 5:**
1. **"The Data Warehouse Toolkit" - Ralph Kimball** (CL√ÅSSICO)
2. **"Star Schema: The Complete Reference" - Christopher Adamson**
3. **"Streaming Systems" - Tyler Akidau** (stream processing)
4. **ClickHouse/Druid/Snowflake official docs** (RICOS)

---

## **FASE 6: TIME-SERIES E STREAMING (4-5 meses)**
*Dados em tempo real*

### **6.1 Time-Series Databases**
- **InfluxDB**: TSM engine, retention policies, continuous queries
- **TimescaleDB**: Hypertables, chunks, compression
- **Prometheus**: Pull model, PromQL, federation
- **VictoriaMetrics**: High cardinality, downsampling

### **6.2 Stream Processing**
- **Kafka**: Partitions, consumer groups, exactly-once semantics
- **Kafka Streams**: Stateful processing, windowing
- **Flink**: Checkpointing, state backends, event time
- **Pulsar**: Multi-tenancy, geo-replication, tiered storage

### **6.3 Event Sourcing e CQRS**
- **Event store**: Immutable event log
- **Projections**: Read models derivados de eventos
- **Snapshots**: Otimizando replay de eventos
- **Saga patterns**: Orquestra√ß√£o de processos longos

### **üìö Livros Fase 6:**
1. **"Kafka: The Definitive Guide" - Neha Narkhede**
2. **"Streaming Systems" - Tyler Akidau** (reprise)
3. **"Time Series Databases" - Ted Dunning**
4. **"Designing Event-Driven Systems" - Ben Stopford**

---

## **FASE 7: BANCOS ESPECIALIZADOS (5-6 meses)**
*Ferramentas certas para problemas certos*

### **7.1 Search Engines**
- **Elasticsearch**: Inverted indexes, sharding, relevance scoring
- **Solr**: Faceting, highlighting, spatial search
- **Lucene internals**: Segments, term dictionary, posting lists
- **Vector search**: Embeddings, HNSW, FAISS

### **7.2 Databases para Gaming**
- **Player data**: Inventory systems, progression
- **Leaderboards**: Sorted sets no Redis, sharding por regi√£o
- **Matchmaking**: Ranking systems (ELO, TrueSkill), lat√™ncia
- **Session storage**: Fast reads/writes, TTL autom√°tico
- **Game state**: CRDT para multiplayer, conflict resolution

### **7.3 Mobile/Edge Databases**
- **SQLite**: Internals, journal modes, WAL
- **Realm**: Object database, sync, reactive queries
- **Couchbase Lite**: Sync gateway, conflict resolution
- **Edge computing**: Sync strategies, offline-first

### **7.4 NewSQL**
- **CockroachDB**: Distributed SQL, Raft, serializable isolation
- **TiDB**: MySQL compatible, TiKV (RocksDB-based)
- **YugabyteDB**: PostgreSQL compatible, DocDB storage
- **Google Spanner**: TrueTime, external consistency

### **üìö Livros Fase 7:**
1. **"Elasticsearch: The Definitive Guide" - Clinton Gormley**
2. **"SQLite Internals" - online resources e source code**
3. **Papers**: CockroachDB, Spanner, Calvin (deterministic DBs)
4. **"Building Mobile Apps at Scale" - Gergely Orosz** (mobile DBs)

---

## **FASE 8: SEGURAN√áA E COMPLIANCE (4-5 meses)**
*Protegendo dados cr√≠ticos*

### **8.1 Authentication & Authorization**
- **RBAC**: Role-based access control, hierarquias
- **Row-level security**: PostgreSQL RLS, Oracle VPD
- **Column-level encryption**: Transparent Data Encryption (TDE)
- **Key management**: KMS, key rotation, HSM

### **8.2 Auditoria e Compliance**
- **Audit logs**: Change tracking, temporal tables
- **GDPR/LGPD**: Right to be forgotten, data portability
- **PCI-DSS**: Requisitos para dados de cart√£o
- **HIPAA**: Dados de sa√∫de, BAA requirements

### **8.3 Backup e Disaster Recovery**
- **Backup strategies**: Full, incremental, differential
- **Point-in-time recovery**: WAL archives, log shipping
- **Cross-region replication**: RTO, RPO targets
- **Disaster recovery testing**: Runbooks, automation

### **8.4 SQL Injection e Defesas**
- **Prepared statements**: Parameteriza√ß√£o adequada
- **ORM security**: Risks, N+1 queries
- **Least privilege**: Minimal permissions necess√°rias
- **Network security**: SSL/TLS, VPN, IP whitelisting

### **üìö Livros Fase 8:**
1. **"Database Security" - Alfred Basta**
2. **"The Database Hacker's Handbook" - David Litchfield**
3. **"Backup & Recovery" - W. Curtis Preston**
4. **OWASP Database Security Cheat Sheet**

---

## **FASE 9: CLOUD E MANAGED SERVICES (5-6 meses)**
*Bancos em cloud computing*

### **9.1 AWS Databases**
- **RDS**: Multi-AZ, read replicas, parameter groups
- **Aurora**: Quorum writes, storage auto-scaling, serverless
- **DynamoDB**: Partition keys, LSI/GSI, streams, DAX
- **DocumentDB, Neptune, Timestream**: Use cases

### **9.2 Google Cloud Databases**
- **Cloud SQL**: HA configs, replication
- **Cloud Spanner**: Splits, hotspotting, interleaving
- **Bigtable**: Row keys, column families, replication
- **Firestore**: Document model, real-time listeners

### **9.3 Azure Databases**
- **Azure SQL**: Elastic pools, hyperscale
- **Cosmos DB**: Multi-model, consistency levels, global distribution
- **Synapse Analytics**: Data warehousing, PolyBase

### **9.4 Serverless Databases**
- **Aurora Serverless**: Auto-scaling, pricing model
- **DynamoDB on-demand**: Capacity modes
- **Firebase**: Real-time sync, offline persistence
- **FaunaDB**: Temporal queries, ACID transactions

### **üìö Livros Fase 9:**
1. **"AWS Database Migration Service" - AWS docs**
2. **"Google Cloud Platform in Action" - JJ Geewax**
3. **"Azure Data Engineering" - Vlad Riscutia**
4. **Cloud provider official documentation** (ESSENCIAL)

---

## **FASE 10: ESPECIALIZA√á√ÉO E RESEARCH (12+ meses cada)**
*Fronteiras do conhecimento*

### **10.1 Database Research**
- **HTAP (Hybrid Transactional/Analytical)**: TiDB, SingleStore
- **Learned indexes**: ML para substituir B-trees
- **Quantum databases**: Primeiros experimentos
- **DNA storage**: Codifica√ß√£o de dados em DNA
- **Serverless query processing**: Photon (Databricks)

### **10.2 Gaming Database Architecture**
- **Brawl Stars-like**: Real-time matchmaking, leaderboards globais
  - Player data sharding por regi√£o/ID
  - Redis para session e matchmaking queues
  - Cassandra para historical stats
  - PostgreSQL para progression e inventory
- **Genshin Impact-like**: Gacha systems, MMO elements
  - Distributed transactions para pulls
  - Event sourcing para audit de rewards
  - Graph DB para social features
  - Time-series para telemetria de gameplay

### **10.3 Blockchain e Distributed Ledgers**
- **Consensus algorithms**: PoW, PoS, DPoS
- **Smart contract storage**: State tries, Merkle Patricia trees
- **IPFS**: Content-addressed storage
- **Decentralized databases**: OrbitDB, GunDB

### **10.4 Advanced Topics**
- **Differential privacy**: Queries com garantias matem√°ticas
- **Homomorphic encryption**: Queries em dados encriptados
- **Federated learning**: ML sem centralizar dados
- **Zero-knowledge proofs**: Verifica√ß√£o sem revela√ß√£o

### **10.5 Building a Database from Scratch**
- **Storage engine**: Implementa√ß√£o completa
- **Query parser e optimizer**: Completo
- **Transaction manager**: MVCC ou 2PL
- **Networking layer**: Wire protocol customizado
- **Replication**: Raft-based consensus

### **üìö Livros Fase 10:**
1. **"Principles of Distributed Database Systems" - M. Tamer √ñzsu**
2. **Research papers**: VLDB, SIGMOD, ICDE conferences
3. **"Blockchain Basics" - Daniel Drescher**
4. **Source code**: PostgreSQL, MySQL, SQLite, RocksDB

### **üõ†Ô∏è Projeto Master Final:**
```
CONSTRUIR UM DATABASE COMPLETO DO ZERO:

- Storage: LSM-tree ou B+tree
- Indexing: M√∫ltiplos tipos
- Query language: SQL subset ou custom DSL
- Transactions: ACID completo
- Replication: Consenso distribu√≠do
- Client protocol: Wire protocol customizado
- Performance: Benchmarks contra DBs reais

Resultado: Portfolio piece que impressiona QUALQUER empresa
```

---

## **METODOLOGIA DE ESTUDO ABSOLUTA**

### **Princ√≠pios Fundamentais:**

#### **1. Teoria + Pr√°tica SEMPRE**
```
- NUNCA estude teoria sem implementar
- Crie projetos reais em CADA fase
- Leia source code de DBs reais
- Contribua para projetos open-source
```

#### **2. Read the Source**
```
- PostgreSQL: Legibilidade excelente, arquitetura cl√°ssica
- SQLite: C√≥digo limpo, √≥timo pra aprender
- Redis: Simplicidade e performance
- RocksDB: LSM-tree implementation reference
```

#### **3. Benchmarking Obsessivo**
```
- Sempre me√ßa antes de otimizar
- Aprenda sysbench, YCSB, TPC-C/TPC-H
- Crie seus pr√≥prios benchmarks
- Documente TUDO: hardware, configura√ß√µes, resultados
```

#### **4. Paper Reading**
```
- Leia 1 paper cl√°ssico por semana m√≠nimo
- Google, Facebook, Amazon publicam gems
- VLDB, SIGMOD: Top conferences
- Implemente algo de cada paper importante
```

### **Cronograma Sugerido:**
- **Fases -1 a 0**: 3-4 meses (foundation SQL + modelagem)
- **Fases 1-2**: 10-14 meses (internals + distribu√≠dos)
- **Fases 3-4**: 11-14 meses (NoSQL + performance)
- **Fases 5-7**: 14-17 meses (analytics + especializados)
- **Fases 8-9**: 9-11 meses (seguran√ßa + cloud)
- **Fase 10**: 2-4 anos (especializa√ß√£o + research)

**TOTAL**: 5-7 anos para dom√≠nio GOD-LEVEL completo

### **Meta Final:**
Ap√≥s este roadmap, voc√™ ser√° capaz de:
- Arquitetar bancos para QUALQUER escala (milh√µes a bilh√µes de usu√°rios)
- Debugar problemas de performance em qualquer DB
- Escolher a tecnologia PERFEITA para cada use case
- Implementar um database do zero
- Contribuir para projetos como PostgreSQL, MySQL, Cassandra
- Arquitetar sistemas como Brawl Stars, Genshin Impact, Netflix
- Ler e entender qualquer paper de database research
- Otimizar queries que outros consideram "imposs√≠veis"

---

## **RECURSOS COMPLEMENTARES**

### **Tools Essenciais:**
- **SQL**: DBeaver, DataGrip, pgAdmin
- **Profiling**: EXPLAIN ANALYZE, pg_stat_statements, pt-query-digest
- **Monitoring**: Prometheus, Grafana, Datadog
- **Benchmarking**: sysbench, YCSB, pgbench
- **Containers**: Docker, Kubernetes para labs
- **IaC**: Terraform para infra de DBs

### **Comunidades:**
- **Database Architects (Discord/Slack groups)**
- **/r/database, /r/PostgreSQL, /r/mongodb**
- **Stack Overflow Database tag**
- **Local meetups**: PostgreSQL, MongoDB, Cassandra
- **Conferences**: VLDB, SIGMOD, PostgresConf, MongoDB.live

### **Blogs Essenciais:**
- **Use The Index, Luke** (Markus Winand)
- **Percona Blog** (MySQL/PostgreSQL)
- **Cockroach Labs Blog** (distributed systems)
- **AWS Database Blog**
- **Martin Kleppmann's blog**

### **Papers Cl√°ssicos (MUST READ):**
1. **"Architecture of a Database System" - Hellerstein et al.**
2. **"Bigtable" - Google (2006)**
3. **"Dynamo" - Amazon (2007)**
4. **"Spanner" - Google (2012)**
5. **"TAO: Facebook's Distributed Data Store"**
6. **"F1: A Distributed SQL Database" - Google**
7. **"Amazon Aurora: Design Considerations" - Verbitski et al.**

---

## **GAMING DATABASE DEEP DIVE**
*Como jogos AAA fazem*

### **Arquitetura Brawl Stars-like:**

```
PLAYER DATA LAYER:
‚îú‚îÄ‚îÄ PostgreSQL (Primary)
‚îÇ   ‚îú‚îÄ‚îÄ Player profiles (name, level, trophies)
‚îÇ   ‚îú‚îÄ‚îÄ Brawler collection (unlocks, power levels)
‚îÇ   ‚îú‚îÄ‚îÄ Progression system (XP, season pass)
‚îÇ   ‚îî‚îÄ‚îÄ Transaction history (purchases, rewards)
‚îÇ
‚îú‚îÄ‚îÄ Redis (Cache + Real-time)
‚îÇ   ‚îú‚îÄ‚îÄ Active sessions (currently online players)
‚îÇ   ‚îú‚îÄ‚îÄ Matchmaking queues (sorted sets por trophies)
‚îÇ   ‚îú‚îÄ‚îÄ Leaderboards (global, regional, club)
‚îÇ   ‚îî‚îÄ‚îÄ Rate limiting (API abuse prevention)
‚îÇ
‚îú‚îÄ‚îÄ Cassandra (Historical)
‚îÇ   ‚îú‚îÄ‚îÄ Match history (billions of matches)
‚îÇ   ‚îú‚îÄ‚îÄ Stats agregadas (win rates, KDA)
‚îÇ   ‚îú‚îÄ‚îÄ Event participation
‚îÇ   ‚îî‚îÄ‚îÄ Telemetry data
‚îÇ
‚îî‚îÄ‚îÄ Kafka + Flink (Event Stream)
    ‚îú‚îÄ‚îÄ Real-time analytics
    ‚îú‚îÄ‚îÄ Cheat detection
    ‚îú‚îÄ‚îÄ Personalization engine
    ‚îî‚îÄ‚îÄ A/B testing metrics

SHARDING STRATEGY:
- Player ID mod N para distribui√ß√£o
- Regional shards (lat√™ncia)
- Hot players em cache dedicado
```

### **Challenges Espec√≠ficos de Gaming:**

1. **Leaderboards em tempo real** (milh√µes de jogadores)
   - Sorted sets no Redis com TTL
   - Batch updates via Kafka
   - Approximate leaderboards para escala

2. **Matchmaking < 1 segundo**
   - Algoritmo de matching em mem√≥ria (Redis)
   - Pre-computed match pools
   - Geolocaliza√ß√£o para lat√™ncia

3. **Inventory systems complexos**
   - JSONB no PostgreSQL para flexibilidade
   - Denormaliza√ß√£o estrat√©gica
   - Event sourcing para auditoria

4. **Gacha/Loot systems com fairness**
   - Pseudorandom com seeds audit√°veis
   - Pity systems em DB
   - Fraud detection via ML

5. **Live events (milh√µes simult√¢neos)**
   - Read replicas com load balancing
   - Cache warming pr√©-evento
   - Graceful degradation

---

## **SISTEMA DE GACHA HIGH-PERFORMANCE**
*Blueprint completo para implementa√ß√£o*

### **Arquitetura do Sistema:**

```
API LAYER (Node.js/Go/Rust):
‚îú‚îÄ‚îÄ POST /gacha/pull
‚îÇ   ‚îú‚îÄ‚îÄ Autentica√ß√£o (JWT)
‚îÇ   ‚îú‚îÄ‚îÄ Rate limiting (Redis)
‚îÇ   ‚îú‚îÄ‚îÄ Valida√ß√£o de recursos (gems/premium currency)
‚îÇ   ‚îî‚îÄ‚îÄ Transaction orchestration
‚îÇ
‚îú‚îÄ‚îÄ GET /gacha/history
‚îÇ   ‚îú‚îÄ‚îÄ Cache layer (Redis)
‚îÇ   ‚îî‚îÄ‚îÄ Pagination eficiente
‚îÇ
‚îî‚îÄ‚îÄ GET /gacha/rates
    ‚îú‚îÄ‚îÄ Cache agressivo (CDN)
    ‚îî‚îÄ‚îÄ Dynamic rate-up banners

DATABASE LAYER:
‚îú‚îÄ‚îÄ PostgreSQL (ACID critical)
‚îÇ   ‚îú‚îÄ‚îÄ players (id, gems, premium_currency)
‚îÇ   ‚îú‚îÄ‚îÄ player_inventory (player_id, item_id, quantity, obtained_at)
‚îÇ   ‚îú‚îÄ‚îÄ gacha_banners (id, name, active, rate_up_items)
‚îÇ   ‚îú‚îÄ‚îÄ gacha_history (player_id, banner_id, items_pulled, timestamp)
‚îÇ   ‚îî‚îÄ‚îÄ gacha_pity (player_id, banner_id, pulls_since_5star)
‚îÇ
‚îî‚îÄ‚îÄ Redis (Performance critical)
    ‚îú‚îÄ‚îÄ Player session cache
    ‚îú‚îÄ‚îÄ Rate limiting counters
    ‚îú‚îÄ‚îÄ Gacha rates cache (hot data)
    ‚îî‚îÄ‚îÄ Anti-duplicate bloom filters

ANALYTICS LAYER:
‚îú‚îÄ‚îÄ ClickHouse/TimescaleDB
‚îÇ   ‚îú‚îÄ‚îÄ Drop rate tracking (real vs expected)
‚îÇ   ‚îú‚îÄ‚îÄ Revenue per banner
‚îÇ   ‚îú‚îÄ‚îÄ Player spending patterns
‚îÇ   ‚îî‚îÄ‚îÄ Fraud detection signals
```

### **Algoritmo Core do Gacha:**

```javascript
async function performGachaPull(playerId, bannerId, pullCount) {
  // 1. Begin transaction (ALL OR NOTHING)
  const transaction = await db.beginTransaction();
  
  try {
    // 2. Validate player resources (with row lock)
    const player = await transaction.query(
      'SELECT gems, premium_currency FROM players WHERE id = $1 FOR UPDATE',
      [playerId]
    );
    
    const cost = calculatePullCost(bannerId, pullCount);
    if (player.gems < cost) {
      throw new Error('Insufficient resources');
    }
    
    // 3. Get pity counter (guaranteed 5-star logic)
    const pity = await transaction.query(
      'SELECT pulls_since_5star FROM gacha_pity WHERE player_id = $1 AND banner_id = $2 FOR UPDATE',
      [playerId, bannerId]
    );
    
    // 4. Load gacha rates (from cache if available)
    const rates = await getRatesWithPity(bannerId, pity.pulls_since_5star);
    
    // 5. Perform pulls with cryptographically secure randomness
    const pulledItems = [];
    let newPityCounter = pity.pulls_since_5star;
    
    for (let i = 0; i < pullCount; i++) {
      // Guaranteed 5-star at 90 pulls (hard pity)
      if (newPityCounter >= 90) {
        pulledItems.push(selectGuaranteed5Star(bannerId));
        newPityCounter = 0;
      } else {
        // Weighted random with soft pity (increasing rates after 75 pulls)
        const item = weightedRandomPull(rates, newPityCounter);
        pulledItems.push(item);
        
        if (item.rarity === 5) {
          newPityCounter = 0;
        } else {
          newPityCounter++;
        }
      }
    }
    
    // 6. Deduct resources
    await transaction.query(
      'UPDATE players SET gems = gems - $1 WHERE id = $2',
      [cost, playerId]
    );
    
    // 7. Add items to inventory (batch insert for performance)
    await transaction.query(
      'INSERT INTO player_inventory (player_id, item_id, quantity, obtained_at) VALUES ...',
      // Batch insert all items
    );
    
    // 8. Update pity counter
    await transaction.query(
      'UPDATE gacha_pity SET pulls_since_5star = $1 WHERE player_id = $2 AND banner_id = $3',
      [newPityCounter, playerId, bannerId]
    );
    
    // 9. Log to history (for audit)
    await transaction.query(
      'INSERT INTO gacha_history (player_id, banner_id, items_pulled, timestamp) VALUES ($1, $2, $3, NOW())',
      [playerId, bannerId, JSON.stringify(pulledItems)]
    );
    
    // 10. Commit transaction
    await transaction.commit();
    
    // 11. Invalidate relevant caches
    await redis.del(`player:${playerId}:inventory`);
    
    // 12. Emit analytics event (async, non-blocking)
    analyticsQueue.push({
      event: 'gacha_pull',
      playerId,
      bannerId,
      items: pulledItems,
      timestamp: Date.now()
    });
    
    return pulledItems;
    
  } catch (error) {
    // Rollback on any error
    await transaction.rollback();
    throw error;
  }
}

// Weighted random with soft pity system
function weightedRandomPull(rates, pityCounter) {
  // Soft pity: increase 5-star rate after 75 pulls
  let adjusted5StarRate = rates.fiveStar;
  if (pityCounter >= 75) {
    // Increase rate by 6% per pull after 75
    adjusted5StarRate += (pityCounter - 74) * 0.06;
  }
  
  // Cryptographically secure random
  const random = crypto.randomBytes(4).readUInt32BE() / 0xFFFFFFFF;
  
  if (random < adjusted5StarRate) {
    return selectFromPool(rates.fiveStarPool);
  } else if (random < adjusted5StarRate + rates.fourStar) {
    return selectFromPool(rates.fourStarPool);
  } else {
    return selectFromPool(rates.threeStarPool);
  }
}
```

### **Performance Optimizations:**

```sql
-- Critical indexes for performance
CREATE INDEX idx_player_inventory_player_id ON player_inventory(player_id);
CREATE INDEX idx_gacha_history_player_banner ON gacha_history(player_id, banner_id);
CREATE INDEX idx_gacha_pity_lookup ON gacha_pity(player_id, banner_id);

-- Partitioning gacha_history by timestamp (billions of rows)
CREATE TABLE gacha_history (
  id BIGSERIAL,
  player_id BIGINT NOT NULL,
  banner_id INT NOT NULL,
  items_pulled JSONB NOT NULL,
  timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW()
) PARTITION BY RANGE (timestamp);

-- Monthly partitions
CREATE TABLE gacha_history_2025_01 PARTITION OF gacha_history
  FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

-- Materialized view for analytics (refresh hourly)
CREATE MATERIALIZED VIEW gacha_stats_hourly AS
SELECT 
  banner_id,
  date_trunc('hour', timestamp) as hour,
  COUNT(*) as total_pulls,
  SUM((items_pulled::jsonb -> 0 ->> 'rarity')::int = 5) as five_star_pulls,
  AVG((items_pulled::jsonb -> 0 ->> 'rarity')::int) as avg_rarity
FROM gacha_history
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY banner_id, date_trunc('hour', timestamp);
```

### **Anti-Fraud Mechanisms:**

```javascript
// 1. Rate limiting per player
async function checkRateLimit(playerId) {
  const key = `gacha:ratelimit:${playerId}`;
  const current = await redis.incr(key);
  
  if (current === 1) {
    await redis.expire(key, 60); // 1 minute window
  }
  
  if (current > 100) { // Max 100 pulls per minute
    throw new Error('Rate limit exceeded');
  }
}

// 2. Detect statistical anomalies
async function detectAnomalies(playerId) {
  const recentPulls = await db.query(`
    SELECT 
      COUNT(*) as total_pulls,
      SUM(CASE WHEN (items_pulled::jsonb -> 0 ->> 'rarity')::int = 5 THEN 1 ELSE 0 END) as five_stars
    FROM gacha_history
    WHERE player_id = $1 
      AND timestamp > NOW() - INTERVAL '1 hour'
  `, [playerId]);
  
  const fiveStarRate = recentPulls.five_stars / recentPulls.total_pulls;
  
  // Flag if player gets 5-stars at 3x expected rate
  if (fiveStarRate > 0.02 && recentPulls.total_pulls > 50) {
    await flagForReview(playerId, 'Abnormal drop rate');
  }
}

// 3. Audit trail immutability (blockchain-style)
async function createAuditHash(pullRecord) {
  const previousHash = await getLastAuditHash();
  const currentHash = crypto
    .createHash('sha256')
    .update(previousHash + JSON.stringify(pullRecord))
    .digest('hex');
  
  await db.query(
    'INSERT INTO gacha_audit_chain (player_id, record_hash, previous_hash) VALUES ($1, $2, $3)',
    [pullRecord.playerId, currentHash, previousHash]
  );
}
```

### **Load Testing & Benchmarks:**

```bash
# Target performance metrics:
- Pull latency: p50 < 50ms, p95 < 150ms, p99 < 300ms
- Throughput: 10,000+ pulls/second per server
- Database connections: Pool of 50-100 connections
- Cache hit rate: > 95% for gacha rates
- Transaction success rate: > 99.99%

# Tools for load testing:
- k6 (load testing)
- Artillery (API testing)
- pgbench (PostgreSQL specific)
```

---
